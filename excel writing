#!/usr/bin/env python3
"""
phrases_to_control_chunks_mpnet.py

- Uses MPNet ('all-mpnet-base-v2') via sentence-transformers (no fallback).
- Hardcode file paths below.
- Outputs an Excel with two sheets:
    1) chunk_phrase_matches
    2) obligation_to_controls

Requirements:
    pip install sentence-transformers pandas openpyxl numpy
"""

import re
import sys
from typing import List, Dict, Tuple
import numpy as np
import pandas as pd

# ---------------------- CONFIG ----------------------
MODEL_NAME = 'all-mpnet-base-v2'
TOP_N_PHRASES = 3
CHUNK_TARGET_CHARS = 500      # target chunk size in chars (approx)
CHUNK_OVERLAP_SENT_RATIO = 0.25  # overlap ratio between chunks (25%)
MIN_SENT_LEN = 20             # merge sentences shorter than this into neighbours
# ----------------------------------------------------

# -------------------- TEXT UTILITIES -----------------
def normalize_text(t) -> str:
    if t is None:
        return ''
    if not isinstance(t, str):
        if pd.isna(t):
            return ''
        t = str(t)
    t = t.replace('\r\n', '\n').replace('\r', '\n')
    t = t.replace('\t', ' ')
    t = re.sub(r'[\u2018\u2019]', "'", t)
    t = re.sub(r'[\u201c\u201d]', '"', t)
    t = re.sub(r'[\u2022\u2023\u25E6\u2043]', '-', t)
    t = re.sub(r'\n+', ' ', t)
    t = re.sub(r'\s+', ' ', t)
    return t.strip()

def sentence_split(text: str) -> List[str]:
    if not text:
        return []
    # naive sentence split: good enough for regulatory text
    sents = re.split(r'(?<=[.!?])\s+', text.strip())
    sents = [s.strip() for s in sents if s.strip()]
    return sents

def merge_short_sentences(sents: List[str], min_len: int = MIN_SENT_LEN) -> List[str]:
    if not sents:
        return []
    out = []
    buf = ''
    for s in sents:
        if len(s) < min_len:
            # merge into buffer
            if buf:
                buf += ' ' + s
            else:
                buf = s
            # if buffer now long enough, push
            if len(buf) >= min_len:
                out.append(buf)
                buf = ''
        else:
            if buf:
                out.append(buf)
                buf = ''
            out.append(s)
    if buf:
        out.append(buf)
    return out

def build_chunks_from_sentences(sents: List[str], target_chars: int = CHUNK_TARGET_CHARS,
                                overlap_ratio: float = CHUNK_OVERLAP_SENT_RATIO) -> List[List[str]]:
    """
    Group sentences into chunks aiming for ~target_chars per chunk.
    Returns list of chunks, where each chunk is a list of sentences (not yet joined).
    Overlap is implemented in sentences (rounded).
    """
    if not sents:
        return []
    chunks = []
    cur = []
    cur_len = 0
    for sent in sents:
        if cur_len + len(sent) + (1 if cur else 0) > target_chars and cur:
            chunks.append(cur)
            # prepare overlap
            overlap_count = max(1, int(len(cur) * overlap_ratio))
            cur = cur[-overlap_count:].copy()
            cur_len = sum(len(x) + 1 for x in cur)
        else:
            cur.append(sent)
            cur_len += len(sent) + (1 if cur else 0)
    if cur:
        chunks.append(cur)
    # ensure we return non-empty chunks
    return [c for c in chunks if c]

def join_chunk_sentences(chunk_sents: List[str]) -> str:
    return ' '.join(chunk_sents).strip()

# -------------------- EMBEDDINGS (MPNet) --------------------
def get_mpnet_embedder(model_name: str = MODEL_NAME):
    try:
        from sentence_transformers import SentenceTransformer
    except Exception as e:
        raise RuntimeError(
            "sentence-transformers not found. Install with:\n"
            "    pip install sentence-transformers\n"
            "This script uses 'all-mpnet-base-v2' only."
        ) from e
    model = SentenceTransformer(model_name)
    def embed(texts: List[str]) -> np.ndarray:
        if not texts:
            return np.zeros((0, model.get_sentence_embedding_dimension()), dtype=float)
        embs = model.encode(texts, show_progress_bar=False, convert_to_numpy=True)
        norms = np.linalg.norm(embs, axis=1, keepdims=True)
        norms[norms == 0] = 1.0
        return embs / norms
    return embed

def cos_sim_matrix(A: np.ndarray, B: np.ndarray) -> np.ndarray:
    if A.size == 0 or B.size == 0:
        return np.zeros((A.shape[0], B.shape[0]))
    return np.dot(A, B.T)

# -------------------- CORE PROCESS --------------------
def phrases_to_controls_pipeline(
    df_controls: pd.DataFrame,
    df_phrases: pd.DataFrame,
    embed_fn,
    top_n_phrases: int = TOP_N_PHRASES
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    # validations
    if 'Control Id' not in df_controls.columns or 'Full Description' not in df_controls.columns:
        raise ValueError("Controls must include columns: 'Control Id', 'Full Description'")
    if 'Obligation id' not in df_phrases.columns or 'Phrase' not in df_phrases.columns:
        raise ValueError("Phrases must include columns: 'Obligation id', 'Phrase'")

    # normalize
    df_controls = df_controls[['Control Id', 'Full Description']].copy()
    df_controls['Full Description'] = df_controls['Full Description'].apply(normalize_text)
    df_phrases = df_phrases[['Obligation id', 'Phrase']].copy()
    df_phrases['Phrase'] = df_phrases['Phrase'].apply(normalize_text)

    # build phrase lists
    phrase_texts = df_phrases['Phrase'].tolist()
    phrase_obl_ids = df_phrases['Obligation id'].tolist()

    # 1) create sentence lists and chunks for each control
    control_chunks_texts: List[str] = []
    control_chunks_meta: List[Dict] = []  # {control_idx, Control Id, full_text, chunk_idx, chunk_sent_index_list}
    for ctrl_idx, row in df_controls.iterrows():
        ctrl_id = row['Control Id']
        full = row['Full Description'] or ''
        sents = sentence_split(full)
        sents = merge_short_sentences(sents)
        chunks_sent_lists = build_chunks_from_sentences(sents)
        if not chunks_sent_lists:
            # put full text as one chunk
            chunks_sent_lists = [[full]] if full else []
        for cidx, chunk_sents in enumerate(chunks_sent_lists):
            chunk_text = join_chunk_sentences(chunk_sents)
            control_chunks_texts.append(chunk_text)
            control_chunks_meta.append({
                'control_idx': int(ctrl_idx),
                'Control Id': ctrl_id,
                'Full Description': full,
                'chunk_idx': int(cidx),
                'chunk_sentences': chunk_sents,
                'chunk_text': chunk_text
            })

    # 2) embed control chunks and phrases
    if control_chunks_texts:
        ctrl_chunks_emb = embed_fn(control_chunks_texts)  # n_chunks x d
    else:
        ctrl_chunks_emb = np.zeros((0, embed_fn([ "a" ]).shape[1]))
    phrase_emb = embed_fn(phrase_texts) if phrase_texts else np.zeros((0, ctrl_chunks_emb.shape[1]))

    # 3) compute control_chunk x phrase similarity
    sim_mat = cos_sim_matrix(ctrl_chunks_emb, phrase_emb)  # n_chunks x n_phrases

    # 4) for each control chunk record top-N phrases and best sentence per phrase
    chunk_rows = []
    # Also build mapping: per (obligation id) -> control best scores (we'll fill after)
    for i, meta in enumerate(control_chunks_meta):
        sims = sim_mat[i] if sim_mat.size else np.array([])
        if sims.size:
            top_idx = np.argsort(sims)[::-1][:top_n_phrases]
        else:
            top_idx = []
        top_phrases_with_scores = []
        # for justifying phrases with an exact sentence: embed sentences of the chunk once
        chunk_sents = meta['chunk_sentences'] if meta.get('chunk_sentences') else [meta['chunk_text']]
        # embed sentences
        sent_embeddings = embed_fn(chunk_sents) if chunk_sents else np.zeros((0, phrase_emb.shape[1]))
        for pi in top_idx:
            score = float(round(float(sims[pi]), 6))
            phrase_text = phrase_texts[pi]
            obl_id = phrase_obl_ids[pi]
            # find best sentence in chunk for this phrase: sim(sentence, phrase)
            if sent_embeddings.size and phrase_emb.size:
                # dot phrase vector with sentence embeddings
                pvec = phrase_emb[pi]  # (d,)
                s_sims = np.dot(sent_embeddings, pvec)  # (n_sents,)
                best_sent_idx = int(np.argmax(s_sims))
                best_sent_text = chunk_sents[best_sent_idx]
                best_sent_score = float(round(float(s_sims[best_sent_idx]), 6))
            else:
                best_sent_text = ''
                best_sent_score = 0.0
            top_phrases_with_scores.append((phrase_text, score, obl_id, best_sent_text, best_sent_score))

        # aggregate obligations for this chunk: best score per obligation id (from top phrases)
        obl_score_map: Dict[str, float] = {}
        obl_sentence_map: Dict[str, Tuple[str, float]] = {}
        for (ptext, pscore, obl, best_sent, best_sent_score) in top_phrases_with_scores:
            if obl in obl_score_map:
                if pscore > obl_score_map[obl]:
                    obl_score_map[obl] = pscore
                    obl_sentence_map[obl] = (best_sent, best_sent_score)
            else:
                obl_score_map[obl] = pscore
                obl_sentence_map[obl] = (best_sent, best_sent_score)
        agg_obligations_sorted = sorted(obl_score_map.items(), key=lambda x: x[1], reverse=True)

        # prepare row with top-3 phrase columns and aggregated obligations
        row = {
            'Control Id': meta['Control Id'],
            'Control Full Description': meta['Full Description'],
            'Chunk Index': meta['chunk_idx'],
            'Chunk Text': meta['chunk_text'],
            'Aggregated_Obligation_Matches': str(agg_obligations_sorted),
        }
        # fill top phrase columns
        for k in range(top_n_phrases):
            col_p = f'Phrase{k+1}'
            col_ps = f'Phrase{k+1}_Score'
            col_po = f'Phrase{k+1}_Obligation'
            col_sent = f'Phrase{k+1}_Best_Sentence'
            col_sent_score = f'Phrase{k+1}_Best_Sentence_Score'
            if k < len(top_phrases_with_scores):
                ptext, pscore, obl, best_sent, best_sent_score = top_phrases_with_scores[k]
                row[col_p] = ptext
                row[col_ps] = pscore
                row[col_po] = obl
                row[col_sent] = best_sent
                row[col_sent_score] = best_sent_score
            else:
                row[col_p] = ''
                row[col_ps] = ''
                row[col_po] = ''
                row[col_sent] = ''
                row[col_sent_score] = ''
        # justification compact
        justification = "; ".join([f"'{p}' ({s:.3f}) -> {o} | best sentence: \"{sent}\" ({ss:.3f})"
                                  for (p,s,o,sent,ss) in top_phrases_with_scores])
        row['Justification_Text'] = justification
        chunk_rows.append(row)

    chunk_matches_df = pd.DataFrame(chunk_rows)

    # 5) Build Obligation -> Controls mapping: for each obligation id, find controls & best score + sentence
    obl_to_ctrl: Dict[str, Dict[str, Dict]] = {}  # {obl: {control_id: {'best_score':score, 'best_sentence':str}}}
    for row in chunk_rows:
        ctrl_id = row['Control Id']
        chunk_text = row['Chunk Text']
        # scan top phrase obligations
        for k in range(top_n_phrases):
            obl = row.get(f'Phrase{k+1}_Obligation')
            score = row.get(f'Phrase{k+1}_Score') or 0.0
            sent = row.get(f'Phrase{k+1}_Best_Sentence') or ''
            if not obl:
                continue
            if obl not in obl_to_ctrl:
                obl_to_ctrl[obl] = {}
            cmap = obl_to_ctrl[obl]
            # if control not present or score better, update
            if ctrl_id not in cmap or float(score) > float(cmap[ctrl_id]['best_score']):
                cmap[ctrl_id] = {'best_score': float(score), 'best_sentence': sent, 'chunk_text': chunk_text}

    # convert mapping to DataFrame rows
    obl_rows = []
    for obl, cmap in obl_to_ctrl.items():
        # sort controls by best_score desc
        ctrl_list = sorted([(cid, info['best_score'], info['best_sentence']) for cid, info in cmap.items()],
                           key=lambda x: x[1], reverse=True)
        compact = "; ".join([f"{cid} ({s:.3f})" for cid, s, _ in ctrl_list])
        obl_rows.append({
            'Obligation id': obl,
            'Matched Controls (ControlId (score))': compact,
            'Matched Controls Detailed': str(ctrl_list)
        })
    obligation_map_df = pd.DataFrame(obl_rows)

    return chunk_matches_df, obligation_map_df

# -------------------- RUN (hardcoded paths) --------------------
if __name__ == '__main__':
    # EDIT these paths
    controls_path = r"C:\Documents\Control.xlsx"   # contains 'Control Id', 'Full Description'
    phrases_path = r"C:\Documents\Phrases.xlsx"    # contains 'Obligation id', 'Phrase'
    output_path = r"C:\Documents\Phrases_to_Controls_Matches_with_sentences.xlsx"

    # load inputs
    try:
        df_controls = pd.read_excel(controls_path, engine='openpyxl')
    except Exception as e:
        print("Error reading controls file:", e)
        sys.exit(1)
    try:
        df_phrases = pd.read_excel(phrases_path, engine='openpyxl')
    except Exception as e:
        print("Error reading phrases file:", e)
        sys.exit(1)

    # embedder
    print("Loading MPNet model (all-mpnet-base-v2). This may take a moment...")
    embed_fn = get_mpnet_embedder(MODEL_NAME)

    print("Computing phrase -> control chunk matches and extracting best sentence justifications...")
    chunk_matches_df, obligation_map_df = phrases_to_controls_pipeline(df_controls, df_phrases, embed_fn, top_n_phrases=TOP_N_PHRASES)

    # save to multi-sheet excel
    try:
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            chunk_matches_df.to_excel(writer, sheet_name='chunk_phrase_matches', index=False)
            obligation_map_df.to_excel(writer, sheet_name='obligation_to_controls', index=False)
    except Exception as e:
        print("Failed to write output:", e)
        sys.exit(1)

    print("Done. Output written to:", output_path)