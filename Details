import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Sample DataFrame with a column 'name' containing textual values
data = {
    'name': [
        'Incorrect value payment submitted by client',
        'Duplicate payment due to system error',
        'Low risk payment issue identified',
        'Overpayment made by client',
        'High risk payment detected'
    ]
}

df = pd.DataFrame(data)

# Initialize lemmatizer
lemmatizer = WordNetLemmatizer()

# List of phrases to search for
search_phrases = ['incorrect', 'duplicate', 'low risk', 'high risk']

# Function to preprocess and extract the required information
def extract_payment_info(text):
    # Preprocess the text: Tokenize and lemmatize words
    words = word_tokenize(text.lower())  # Convert to lower case and tokenize
    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize words

    # Initialize variables for checking conditions
    found_payment = None
    extracted_phrases = []

    # Check if 'payment', 'overpayment', or 'underpayment' is in the text
    if 'payment' in lemmatized_words or 'overpayment' in lemmatized_words or 'underpayment' in lemmatized_words:
        found_payment = 'payment'

    # Check for presence of other phrases
    for phrase in search_phrases:
        if phrase in lemmatized_words:
            extracted_phrases.append(phrase)
    
    # If 'payment' found, add it to the list of extracted phrases
    if found_payment:
        extracted_phrases.append(found_payment)
    
    # Return extracted phrases as a comma-separated string
    return ', '.join(extracted_phrases) if extracted_phrases else None

# Apply the function to the 'name' column and create a new column 'extracted_info'
df['extracted_info'] = df['name'].apply(extract_payment_info)

# Display the DataFrame
print(df)