import re
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Make sure required NLTK packages are downloaded
nltk.download('punkt')
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()

def preprocess_and_count(text):
    # Step 1: Remove special characters and underscores
    cleaned_text = re.sub(r"(\W+)|(_)", " ", text)
    
    # Step 2: Tokenize and Lemmatize
    tokens = word_tokenize(cleaned_text)
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]
    
    # Step 3: Word Count (after lemmatization)
    word_count = len(lemmatized_tokens)
    
    return word_count