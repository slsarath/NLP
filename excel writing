#!/usr/bin/env python3
"""
match_controls_obligations_mpnet_controlphrases.py

- Uses MPNet ('all-mpnet-base-v2') from sentence-transformers (no fallback).
- Hardcoded paths at bottom (edit before running).
- Output: one row per (Obligation, matched Control) — up to top 3 controls per obligation.
- Semantic phrases are compared ONLY with control chunks (not obligation chunks).
"""

import re
import sys
from typing import List, Tuple
import numpy as np
import pandas as pd

MODEL_NAME = 'all-mpnet-base-v2'
TOP_K = 3
CHUNK_MAX_CHARS = 700

# --- Your semantic phrases (assumed derived from obligations) ---
SEMANTIC_PHRASES = [
    'LYOD cannot terminate an account before its expiration date if no finance charge is incurred',
    'Account termination cannot be based solely on absence of finance charges',
    'LYOD may terminate accounts inactive for three consecutive months',
    'Termination allowed for accounts with prolonged inactivity',
    'Conditions under which LYOD can or cannot close accounts:',
    'Account closure policy regarding expiration date, finance charges, and inactivity',
    'Regulatory requirement for account termination policies',
    'Exceptions to account termination restrictions for inactivity',
    'Rules for record retention and inspection under Regulation Z',
    'Compliance documentation requirements and CFPB oversight',
    'LYOD obligations for maintaining and providing compliance evidence',
    'Procedures for retaining and sharing compliance records with regulators',
    'LYOD responsibilities for Reg Z compliance evidence'
]

# -------------------------
# Text utilities
# -------------------------
def normalize_text(t) -> str:
    if t is None:
        return ''
    if not isinstance(t, str):
        if pd.isna(t):
            return ''
        t = str(t)
    t = t.replace('\r\n', '\n').replace('\r', '\n')
    t = t.replace('\t', ' ')
    t = re.sub(r'\u2019|\u2018', "'", t)
    t = re.sub(r'\u201c|\u201d', '"', t)
    t = re.sub(r'[\u2022\u2023\u25E6\u2043]', '-', t)  # bullets -> dash
    t = re.sub(r'\n+', ' ', t)
    t = re.sub(r'\s+', ' ', t)
    return t.strip()

def sentence_chunker(text: str, max_chars: int = CHUNK_MAX_CHARS) -> List[str]:
    text = (text or '').strip()
    if not text:
        return []
    sents = re.split(r'(?<=[.!?])\s+', text)
    chunks = []
    buf = []
    buf_len = 0
    for s in sents:
        s = s.strip()
        if not s:
            continue
        if buf_len + len(s) + (1 if buf else 0) > max_chars and buf:
            chunks.append(' '.join(buf))
            buf = [s]
            buf_len = len(s)
        else:
            buf.append(s)
            buf_len += len(s) + (1 if buf else 0)
    if buf:
        chunks.append(' '.join(buf))
    return [c for c in chunks if c]

# -------------------------
# Embeddings (MPNet only)
# -------------------------
def get_mpnet_embedder(model_name: str = MODEL_NAME):
    try:
        from sentence_transformers import SentenceTransformer
    except Exception as e:
        raise RuntimeError(
            "sentence-transformers is required and not found. Install with:\n"
            "    pip install sentence-transformers\n"
            "This script uses 'all-mpnet-base-v2' and will not fallback to TF-IDF."
        ) from e
    model = SentenceTransformer(model_name)
    def embed(texts: List[str]) -> np.ndarray:
        if not texts:
            return np.zeros((0, model.get_sentence_embedding_dimension()), dtype=float)
        embs = model.encode(texts, show_progress_bar=False, convert_to_numpy=True)
        norms = np.linalg.norm(embs, axis=1, keepdims=True)
        norms[norms == 0] = 1.0
        return embs / norms
    return embed

def cos_sim_matrix(A: np.ndarray, B: np.ndarray) -> np.ndarray:
    if A.size == 0 or B.size == 0:
        return np.zeros((A.shape[0], B.shape[0]))
    return np.dot(A, B.T)

# -------------------------
# Phrase matching: now only control_chunk -> phrases
# -------------------------
def top_phrases_for_control_chunk(
    embed_fn,
    control_chunk: str,
    phrases: List[str],
    phrase_emb: np.ndarray,
    top_n: int = 3
) -> Tuple[List[Tuple[str, float]], str]:
    """
    Returns top_n phrase matches (phrase, score) where score = sim(control_chunk, phrase)
    and a human-readable justification string.
    """
    if not phrases:
        return [], ''
    c_emb = embed_fn([control_chunk])[0]  # vector
    # similarity: phrase_emb (P x D) dot c_emb (D,) => (P,)
    c2p = np.dot(phrase_emb, c_emb)
    order = np.argsort(c2p)[::-1][:top_n]
    top = [(phrases[i], float(round(c2p[i], 4))) for i in order]
    justification = '; '.join([f"'{p}' ({s:.3f})" for p, s in top])
    return top, justification

# -------------------------
# Core matching logic (modified to only compare phrases with control chunks)
# -------------------------
def produce_matches(
    df_controls: pd.DataFrame,
    df_obligations: pd.DataFrame,
    embed_fn
) -> pd.DataFrame:
    controls = df_controls[['Control Id', 'Full Description']].copy()
    obligations = df_obligations[['Obligation id', 'Detailed obligation description']].copy()

    controls['clean'] = controls['Full Description'].apply(normalize_text)
    obligations['clean'] = obligations['Detailed obligation description'].apply(normalize_text)

    # full-text embeddings
    ctrl_texts = controls['clean'].tolist()
    ob_texts = obligations['clean'].tolist()
    ctrl_emb = embed_fn(ctrl_texts)
    ob_emb = embed_fn(ob_texts)

    # chunk controls and obligations and keep metadata
    control_chunks = []
    control_chunks_meta = []  # (control_row_index, chunk_text)
    for i, txt in enumerate(controls['clean'].tolist()):
        chunks = sentence_chunker(txt)
        if not chunks:
            chunks = [txt]
        for ch in chunks:
            control_chunks.append(ch)
            control_chunks_meta.append((i, ch))

    obligation_chunks = []
    obligation_chunks_meta = []  # (obligation_row_index, chunk_text)
    for j, txt in enumerate(obligations['clean'].tolist()):
        chunks = sentence_chunker(txt)
        if not chunks:
            chunks = [txt]
        for ch in chunks:
            obligation_chunks.append(ch)
            obligation_chunks_meta.append((j, ch))

    # embed chunks
    ctrl_chunks_emb = embed_fn(control_chunks) if control_chunks else np.zeros((0, ctrl_emb.shape[1]))
    ob_chunks_emb = embed_fn(obligation_chunks) if obligation_chunks else np.zeros((0, ctrl_emb.shape[1]))

    # embed phrases (once) — used ONLY vs control chunks
    phrase_emb = embed_fn(SEMANTIC_PHRASES)

    # full-text similarity matrix (obligation x control)
    full_sim = cos_sim_matrix(ob_emb, ctrl_emb)  # shape (n_ob, n_ctrl)

    results = []
    n_ob = ob_emb.shape[0]
    for ob_idx in range(n_ob):
        ob_id = obligations.loc[ob_idx, 'Obligation id']
        ob_full_text = obligations.loc[ob_idx, 'clean']
        sims = full_sim[ob_idx]  # similarity to every control
        top_ctrl_positions = np.argsort(sims)[::-1][:TOP_K]
        for pos in top_ctrl_positions:
            direct_sim = float(round(float(sims[pos]), 4))
            ctrl_id = controls.loc[pos, 'Control Id']
            ctrl_full_text = controls.loc[pos, 'clean']

            # find best chunk pair between this obligation and this control (for chunk similarity and context)
            best_chunk_sim = 0.0
            best_ctrl_chunk = ''
            best_ob_chunk = ''

            ob_chunk_positions = [i for i, (orig_idx, _) in enumerate(obligation_chunks_meta) if orig_idx == ob_idx]
            ctrl_chunk_positions = [i for i, (orig_idx, _) in enumerate(control_chunks_meta) if orig_idx == pos]

            if ob_chunk_positions and ctrl_chunk_positions:
                sub_ob_emb = ob_chunks_emb[ob_chunk_positions, :]   # m x d
                sub_ctrl_emb = ctrl_chunks_emb[ctrl_chunk_positions, :]  # n x d
                pair_sim = cos_sim_matrix(sub_ctrl_emb, sub_ob_emb)  # n x m
                i_flat, j_flat = np.unravel_index(np.argmax(pair_sim, axis=None), pair_sim.shape)
                best_chunk_sim = float(round(float(pair_sim[i_flat, j_flat]), 4))
                global_ctrl_chunk_pos = ctrl_chunk_positions[i_flat]
                global_ob_chunk_pos = ob_chunk_positions[j_flat]
                best_ctrl_chunk = control_chunks_meta[global_ctrl_chunk_pos][1]
                best_ob_chunk = obligation_chunks_meta[global_ob_chunk_pos][1]
            else:
                # if no chunk mapping (rare), set best_ctrl_chunk to empty and still compute phrases from control full text
                best_ctrl_chunk = ''
                best_ob_chunk = ''

            # PHRASE MATCHING: only compare phrases with control chunk (or control full text if chunk empty)
            phrase_source_text = best_ctrl_chunk if best_ctrl_chunk else ctrl_full_text
            top_phrases, phrase_just = top_phrases_for_control_chunk(
                embed_fn=embed_fn,
                control_chunk=phrase_source_text,
                phrases=SEMANTIC_PHRASES,
                phrase_emb=phrase_emb,
                top_n=3
            )

            results.append({
                'Obligation ID': ob_id,
                'Obligation Text': ob_full_text,
                'Matched Control ID': ctrl_id,
                'Matched Control Text': ctrl_full_text,
                'Direct Similarity': direct_sim,
                'Best Chunk Similarity': best_chunk_sim,
                'Best Obligation Chunk Text': best_ob_chunk,
                'Best Control Chunk Text': best_ctrl_chunk,
                'Top 3 Phrase Matches (phrase, score)': str([(p, round(s,4)) for p, s in top_phrases]),
                'Phrase Justification Text': phrase_just
            })

    return pd.DataFrame(results)

# -------------------------
# Run: hardcoded paths
# -------------------------
if __name__ == '__main__':
    # ---- EDIT THESE PATHS BEFORE RUNNING ----
    controls_path = r"C:\Documents\Control.xlsx"
    obligations_path = r"C:\Documents\LegalObligations.xlsx"
    output_path = r"C:\Documents\Matched_Output.xlsx"
    # ----------------------------------------

    try:
        df_controls = pd.read_excel(controls_path, engine='openpyxl')
    except Exception as e:
        print("Error reading controls file:", e)
        sys.exit(1)
    try:
        df_obligations = pd.read_excel(obligations_path, engine='openpyxl')
    except Exception as e:
        print("Error reading obligations file:", e)
        sys.exit(1)

    # required columns check
    if 'Control Id' not in df_controls.columns or 'Full Description' not in df_controls.columns:
        print("Controls Excel must contain columns: 'Control Id' and 'Full Description'")
        print("Found:", list(df_controls.columns))
        sys.exit(1)
    if 'Obligation id' not in df_obligations.columns or 'Detailed obligation description' not in df_obligations.columns:
        print("Obligations Excel must contain columns: 'Obligation id' and 'Detailed obligation description'")
        print("Found:", list(df_obligations.columns))
        sys.exit(1)

    print("Loading MPNet model (all-mpnet-base-v2). This may take a moment...")
    embed_fn = get_mpnet_embedder(MODEL_NAME)

    print("Computing matches (top 3 controls per obligation)...")
    out_df = produce_matches(df_controls, df_obligations, embed_fn)

    try:
        out_df.to_excel(output_path, index=False)
    except Exception as e:
        print("Failed to write output:", e)
        sys.exit(1)

    print(f"Done. Output saved to: {output_path}")