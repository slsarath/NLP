from sentence_transformers import SentenceTransformer
from sklearn.cluster import AgglomerativeClustering

# Step 1: Split phrases into list
def split_phrases(text):
    if pd.isna(text) or text.strip() == "":
        return []
    return [p.strip() for p in text.split(",") if p.strip()]

df['TFIDF_Phrase_List'] = df['TFIDF_Themes'].apply(split_phrases)
df['KeyBERT_Phrase_List'] = df['KeyBERT_Themes'].apply(split_phrases)

# Step 2: Get unique phrases from both methods
tfidf_all_phrases = set(phrase for phrases in df['TFIDF_Phrase_List'] for phrase in phrases)
keybert_all_phrases = set(phrase for phrases in df['KeyBERT_Phrase_List'] for phrase in phrases)

# Step 3: Embed each unique phrase
model = SentenceTransformer('all-MiniLM-L6-v2')
tfidf_embeddings = model.encode(list(tfidf_all_phrases), show_progress_bar=True)
keybert_embeddings = model.encode(list(keybert_all_phrases), show_progress_bar=True)

# Step 4: Cluster phrases with flexible threshold
def cluster_phrases(phrases, embeddings, distance=0.7):
    model = AgglomerativeClustering(n_clusters=None, distance_threshold=distance, metric='cosine', linkage='average')
    labels = model.fit_predict(embeddings)
    return dict(zip(phrases, labels))

tfidf_phrase_to_cluster = cluster_phrases(list(tfidf_all_phrases), tfidf_embeddings, distance=0.7)
keybert_phrase_to_cluster = cluster_phrases(list(keybert_all_phrases), keybert_embeddings, distance=0.7)

# Step 5: Map each row's phrases to their clusters
def map_clusters(phrases, mapping):
    return ",".join(str(mapping[p]) for p in phrases if p in mapping)

df['TFIDF_Cluster'] = df['TFIDF_Phrase_List'].apply(lambda x: map_clusters(x, tfidf_phrase_to_cluster))
df['KeyBERT_Cluster'] = df['KeyBERT_Phrase_List'].apply(lambda x: map_clusters(x, keybert_phrase_to_cluster))