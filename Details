import pandas as pd
import re
import nltk
import numpy as np
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from keybert import KeyBERT

# Optional: for visualizing clusters
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Download NLTK stopwords if not already done
nltk.download('stopwords')

# === Step 1: Load Data ===
df = pd.read_csv("negative_transcripts.csv")  # Replace with your path

# === Step 2: Filter for Negative Customer Utterances ===
df = df[
    (df['Participant_id'].str.lower() == 'customer') &
    (df['Sentiment'].str.lower() == 'negative')
]

# === Step 3: Text Cleaning ===
default_stopwords = set(stopwords.words('english'))
custom_stopwords = {"ess", "ffr"}
all_stopwords = default_stopwords.union(custom_stopwords)

def clean_text(text):
    text = re.sub(r'[^a-zA-Z\s]', ' ', str(text).lower())
    tokens = [word for word in text.split() if word not in all_stopwords]
    return " ".join(tokens)

# Clean transcripts and store in new column
df['Cleaned'] = df['Transcript'].apply(lambda x: clean_text(x) if isinstance(x, str) else "")

# Filter rows where Cleaned text is not empty
df = df[df['Cleaned'].str.strip() != ""].reset_index(drop=True)

# === Step 4: Sentence Embedding ===
model = SentenceTransformer('all-MiniLM-L6-v2')
utterances = df['Cleaned'].tolist()
embeddings = model.encode(utterances, show_progress_bar=True)

# === Step 5: Clustering ===
n_clusters = 8  # You can tune this
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
labels = kmeans.fit_predict(embeddings)
df['Cluster'] = labels

# === Step 6: Extract Theme Phrases Per Cluster ===
kw_model = KeyBERT(model='all-MiniLM-L6-v2')
theme_phrases = {}

for cluster_id in sorted(df['Cluster'].unique()):
    texts = df[df['Cluster'] == cluster_id]['Cleaned']
    combined_text = " ".join(texts)
    keyphrases = kw_model.extract_keywords(
        combined_text,
        keyphrase_ngram_range=(2, 4),
        stop_words=list(all_stopwords),
        top_n=3
    )
    # Store phrases as comma-separated string
    theme_phrases[cluster_id] = ", ".join([phrase for phrase, _ in keyphrases])

# === Step 7: Map Theme Phrases Back to Each Row ===
df['Theme_Phrases'] = df['Cluster'].map(theme_phrases)

# === Step 8: (Optional) View a Sample ===
print(df[['Transcript', 'Theme_Phrases']].head(10))

# === Step 9: (Optional) Export to CSV ===
# df.to_csv("labeled_theme_transcripts.csv", index=False)