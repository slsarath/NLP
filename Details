import os
import extract_msg
import pandas as pd
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Initialize the lemmatizer
lemmatizer = WordNetLemmatizer()

# Define the words/phrases list
word_list = ["example", "test", "document"]

# Convert the list to include both singular and plural forms
def generate_word_forms(word_list):
    word_forms = []
    for word in word_list:
        word_forms.append(word)
        word_forms.append(lemmatizer.lemmatize(word, 'n'))  # singular form
        word_forms.append(lemmatizer.lemmatize(word, 'n') + 's')  # plural form
    return set(word_forms)

word_forms = generate_word_forms(word_list)

# Function to read .msg files and check for word matches
def search_in_msg_file(file_path, word_forms):
    try:
        msg = extract_msg.Message(file_path)
        message = msg.body.lower()

        tokens = word_tokenize(message)
        found_words = set(tokens) & word_forms

        return list(found_words) if found_words else []
    except Exception as e:
        print(f"Error processing {file_path}: {e}")
        return []

# Function to traverse directories and subdirectories and collect data
def traverse_and_search(root_folder, word_forms):
    data = []
    for root, dirs, files in os.walk(root_folder):
        for file in files:
            if file.endswith(".msg"):
                file_path = os.path.join(root, file)
                found_words = search_in_msg_file(file_path, word_forms)
                data.append({"File Name": file, "Search Results": found_words})
    return data

# Path to the root folder containing .msg files
root_folder = '/path/to/your/folder'

# Traverse the folder and check for matching words, storing results in a list
data = traverse_and_search(root_folder, word_forms)

# Convert the list of results into a DataFrame
df = pd.DataFrame(data)

# Display the DataFrame
print(df)

# Optionally, save the DataFrame to a CSV file
df.to_csv("search_results.csv", index=False)