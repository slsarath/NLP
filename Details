import os
import extract_msg
import pandas as pd
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Initialize the lemmatizer
lemmatizer = WordNetLemmatizer()

# Define the words/phrases list
word_list = ["example", "test", "document"]

# Convert the list to include both singular and plural forms
def generate_word_forms(word_list):
    word_forms = []
    for word in word_list:
        word_forms.append(word)
        word_forms.append(lemmatizer.lemmatize(word, 'n'))  # singular form
        word_forms.append(lemmatizer.lemmatize(word, 'n') + 's')  # plural form
    return set(word_forms)

word_forms = generate_word_forms(word_list)

# Function to read .msg files and check for word matches
def search_in_msg_file(file_path, word_forms):
    msg = extract_msg.Message(file_path)
    message = msg.body.lower()

    tokens = word_tokenize(message)
    found_words = set(tokens) & word_forms

    return list(found_words)

# Function to traverse directories and subdirectories
def traverse_and_search(root_folder, word_forms):
    results = []
    for root, dirs, files in os.walk(root_folder):
        for file in files:
            if file.endswith(".msg"):
                file_path = os.path.join(root, file)
                found_words = search_in_msg_file(file_path, word_forms)
                if found_words:
                    results.append({"File Path": file_path, "Found Words": ", ".join(found_words)})
    
    return results

# Path to the root folder containing .msg files
root_folder = '/path/to/your/folder'

# Traverse the folder and check for matching words
results = traverse_and_search(root_folder, word_forms)

# Convert the results to a DataFrame
df_results = pd.DataFrame(results)

# Display the DataFrame
print(df_results)